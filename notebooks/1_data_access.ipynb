{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Clone or update the repo, then install only what Colab lacks\n",
        "!rm -rf technosignature-pipeline-v2\n",
        "!git clone https://github.com/rbaner21/technosignature-pipeline-v2.git\n",
        "!pip install -q s3fs astroquery lightkurve papermill pyarrow pandas\n"
      ],
      "metadata": {
        "id": "NLeE4hmM8Av3",
        "outputId": "fe0f2130-a014-4f24-b899-dfaad7457d9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "NLeE4hmM8Av3",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'technosignature-pipeline-v2'...\n",
            "remote: Enumerating objects: 81, done.\u001b[K\n",
            "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 81 (delta 28), reused 23 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (81/81), 28.50 KiB | 2.04 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Pull the confirmed‑planets table\n",
        "url = (\n",
        "    \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync\"\n",
        "    \"?query=select+*+from+pscomppars&format=csv\"\n",
        ")\n",
        "pl = pd.read_csv(url)\n",
        "pl.to_csv(\"inputs/confirmed_planets.csv\", index=False)\n",
        "print(f\"✅ Confirmed planets: {len(pl)} rows saved\")\n"
      ],
      "metadata": {
        "id": "KDq5iigJQKK1"
      },
      "id": "KDq5iigJQKK1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pyarrow.dataset as ds\n",
        "import pyarrow.fs    as fs\n",
        "\n",
        "# Prepare output\n",
        "os.makedirs(\"inputs\", exist_ok=True)\n",
        "neo_out = \"inputs/neowise_filtered.parquet\"\n",
        "if os.path.exists(neo_out): os.remove(neo_out)\n",
        "\n",
        "# Point at the public NEOWISE Parquet bucket\n",
        "s3 = fs.S3FileSystem(region=\"us-west-2\", anonymous=True)\n",
        "dataset = ds.dataset(\n",
        "    \"s3://nasa-irsa-wise/wise/neowiser\",\n",
        "    filesystem=s3,\n",
        "    format=\"parquet\",\n",
        "    partitioning=\"hive\"\n",
        ")\n",
        "\n",
        "# Scan in 100k‑row batches, computing W2–W1 > 0.5 flag\n",
        "scanner = dataset.scanner(\n",
        "    columns=[\"ra\",\"dec\",\"w1mpro\",\"w2mpro\"],\n",
        "    batch_size=100_000\n",
        ")\n",
        "for i,batch in enumerate(scanner.scan_batches()):\n",
        "    df = batch.to_pandas()\n",
        "    df[\"ir_flag\"] = (df[\"w2mpro\"] - df[\"w1mpro\"]) > 0.5\n",
        "    flagged = df[df[\"ir_flag\"]]\n",
        "    flagged.to_parquet(neo_out, index=False, append=True)\n",
        "    print(f\"Batch {i:03d}: scanned {len(df)}, flagged {len(flagged)}\")\n",
        "print(\"✅ NEOWISE IR‐excess streaming complete\")\n"
      ],
      "metadata": {
        "id": "ssObBcr3QRHK"
      },
      "id": "ssObBcr3QRHK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pandas as pd\n",
        "\n",
        "# Download full BL hits CSV (one time)\n",
        "url = \"https://public.breakthroughlisten.org/dataset/hits.csv\"\n",
        "pd.read_csv(url, nrows=0).to_csv  # just test URL\n",
        "!wget -qO inputs/bl_hits.csv {url}\n",
        "\n",
        "# Stream & flag 1 420 MHz hits\n",
        "bl_in  = \"inputs/bl_hits.csv\"\n",
        "bl_out = \"inputs/bl_filtered.csv\"\n",
        "if os.path.exists(bl_out): os.remove(bl_out)\n",
        "\n",
        "for chunk in pd.read_csv(bl_in, chunksize=1_000_000):\n",
        "    mask = chunk[\"frequency_mhz\"].between(1419,1421)\n",
        "    chunk[mask].to_csv(\n",
        "        bl_out,\n",
        "        mode=\"a\",\n",
        "        index=False,\n",
        "        header=not os.path.exists(bl_out)\n",
        "    )\n",
        "    print(f\"Processed {len(chunk)}, kept {mask.sum()}\")\n",
        "print(\"✅ BL radio‐hit streaming complete\")\n"
      ],
      "metadata": {
        "id": "Fi7LtM9RQUuy"
      },
      "id": "Fi7LtM9RQUuy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from astroquery.mast import Observations\n",
        "import lightkurve as lk\n",
        "from astropy.timeseries import BoxLeastSquares\n",
        "\n",
        "pl = pd.read_csv(\"inputs/confirmed_planets.csv\")\n",
        "flags = []\n",
        "for tic in pl[\"tic_id\"].dropna().unique():\n",
        "    try:\n",
        "        # Download & flatten\n",
        "        lc = lk.search_lightcurve(f\"TIC {int(tic)}\", mission=\"TESS\").download()\n",
        "        lc_flat = lc.flatten(window_length=401)\n",
        "        # BoxLeastSquares power\n",
        "        bls = BoxLeastSquares(lc_flat.time, lc_flat.flux)\n",
        "        periods = np.linspace(0.5, 10, 5000)\n",
        "        power  = bls.power(periods, 0.1).power\n",
        "        depth  = bls.depth.max()\n",
        "        snr    = power.max() / np.std(lc_flat.flux)\n",
        "        flags.append({\n",
        "            \"tic_id\":    tic,\n",
        "            \"tess_flag\": bool((depth > 0.05) & (snr > 20)),\n",
        "            \"tess_period\": periods[np.argmax(power)],\n",
        "            \"tess_snr\":    float(snr)\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"TIC {tic} error:\", e)\n",
        "\n",
        "pd.DataFrame(flags).to_csv(\"inputs/tess_flags.csv\", index=False)\n",
        "print(f\"✅ TESS flagging complete ({len(flags)} objects)\")\n"
      ],
      "metadata": {
        "id": "RxT6zLjCQWr8"
      },
      "id": "RxT6zLjCQWr8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from astroquery.gaia import Gaia\n",
        "import pandas as pd\n",
        "\n",
        "pl = pd.read_csv(\"inputs/confirmed_planets.csv\")\n",
        "gaia_flags = []\n",
        "for _,row in pl.iterrows():\n",
        "    ra,dec = row[\"ra\"], row[\"dec\"]\n",
        "    q = f\"\"\"\n",
        "      SELECT ruwe\n",
        "      FROM gaiaedr3.gaia_source\n",
        "      WHERE CONTAINS(\n",
        "        POINT('ICRS',ra,dec),\n",
        "        CIRCLE('ICRS',{ra},{dec},0.00027778)\n",
        "      )=1\n",
        "      LIMIT 1\n",
        "    \"\"\"\n",
        "    try:\n",
        "        res = Gaia.launch_job(q).get_results().to_pandas()\n",
        "        ruwe = res[\"ruwe\"].iloc[0] if not res.empty else None\n",
        "        gaia_flags.append({\n",
        "            \"planet_id\": row[\"pl_name\"],\n",
        "            \"gaia_flag\": bool(ruwe and ruwe>1.4),\n",
        "            \"ruwe\":       float(ruwe) if ruwe else None\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(\"Gaia error:\", e)\n",
        "\n",
        "pd.DataFrame(gaia_flags).to_csv(\"inputs/gaia_flags.csv\", index=False)\n",
        "print(f\"✅ Gaia RUWE flagging complete ({len(gaia_flags)} objects)\")\n"
      ],
      "metadata": {
        "id": "_pIV0DabQYc-"
      },
      "id": "_pIV0DabQYc-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from astroquery.eso import Eso\n",
        "import pandas as pd, os, glob\n",
        "\n",
        "# 1) Download one HARPS file (example)\n",
        "eso = Eso()\n",
        "# eso.login()  # if you have ESO creds\n",
        "prod = eso.query_program(\"HARPS\")[0]\n",
        "eso.retrieve_data(prod, output_dir=\"inputs/harps/\")\n",
        "\n",
        "# 2) Stream all HARPS CSVs and flag spikes\n",
        "spec_flags = []\n",
        "for f in glob.glob(\"inputs/harps/*.csv\"):\n",
        "    df = pd.read_csv(f)\n",
        "    spike_count = (df[\"flux\"] > 5*df[\"flux\"].median()).sum()\n",
        "    spec_flags.append({\n",
        "        \"file\":       os.path.basename(f),\n",
        "        \"spike_flag\": bool(spike_count>0),\n",
        "        \"spike_count\": int(spike_count)\n",
        "    })\n",
        "\n",
        "pd.DataFrame(spec_flags).to_csv(\"inputs/spec_flags.csv\", index=False)\n",
        "print(f\"✅ HARPS flagging complete ({len(spec_flags)} files)\")\n"
      ],
      "metadata": {
        "id": "VJvbPJMwQeKH"
      },
      "id": "VJvbPJMwQeKH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}