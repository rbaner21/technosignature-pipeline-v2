{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: ensure inputs/ hierarchy exists\n",
        "import os\n",
        "os.makedirs(\"inputs/tess\",  exist_ok=True)\n",
        "os.makedirs(\"inputs/harps\", exist_ok=True)\n",
        "print(\"✅ inputs/ folder ready\")\n"
      ],
      "metadata": {
        "id": "NLeE4hmM8Av3",
        "outputId": "fe0f2130-a014-4f24-b899-dfaad7457d9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "NLeE4hmM8Av3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'technosignature-pipeline-v2'...\n",
            "remote: Enumerating objects: 81, done.\u001b[K\n",
            "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 81 (delta 28), reused 23 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (81/81), 28.50 KiB | 2.04 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: download list of confirmed exoplanets\n",
        "import pandas as pd\n",
        "\n",
        "url = (\n",
        "    \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync\"\n",
        "    \"?query=select+pl_name,ra,dec,tic_id+from+pscomppars&format=csv\"\n",
        ")\n",
        "pl = pd.read_csv(url)\n",
        "pl.to_csv(\"inputs/confirmed_planets.csv\", index=False)\n",
        "print(f\"✅ Confirmed planets: {len(pl)} rows\")\n"
      ],
      "metadata": {
        "id": "KDq5iigJQKK1"
      },
      "id": "KDq5iigJQKK1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Parallel NEOWISE Single‑Exposure → inputs/neowise.csv\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from astropy.coordinates import SkyCoord\n",
        "import astropy.units as u\n",
        "from astroquery.irsa import Irsa\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Prepare\n",
        "os.makedirs(\"inputs\", exist_ok=True)\n",
        "out = \"inputs/neowise.csv\"\n",
        "if os.path.exists(out):\n",
        "    os.remove(out)\n",
        "\n",
        "planets = pd.read_csv(\"inputs/confirmed_planets.csv\")\n",
        "\n",
        "def query_neowise(row):\n",
        "    coord = SkyCoord(ra=row[\"ra\"]*u.deg, dec=row[\"dec\"]*u.deg, frame=\"icrs\")\n",
        "    try:\n",
        "        tbl = Irsa.query_region(\n",
        "            coord,\n",
        "            catalog=\"neowiser_p1bs_psd\",\n",
        "            radius=5*u.arcsec,\n",
        "            columns=[\"w1mpro\",\"w2mpro\"]\n",
        "        )\n",
        "        if len(tbl):\n",
        "            w1s = np.array(tbl[\"w1mpro\"], float)\n",
        "            w2s = np.array(tbl[\"w2mpro\"], float)\n",
        "            w1_med = float(np.nanmedian(w1s))\n",
        "            w2_med = float(np.nanmedian(w2s))\n",
        "            return {\n",
        "                \"pl_name\": row[\"pl_name\"],\n",
        "                \"w1_med\":  w1_med,\n",
        "                \"w2_med\":  w2_med,\n",
        "                \"ir_flag\": bool((w2_med - w1_med) > 0.5)\n",
        "            }\n",
        "    except:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "results = []\n",
        "with ThreadPoolExecutor(max_workers=16) as exe:\n",
        "    futures = {exe.submit(query_neowise, r): r for _, r in planets.iterrows()}\n",
        "    for f in as_completed(futures):\n",
        "        r = f.result()\n",
        "        if r:\n",
        "            results.append(r)\n",
        "\n",
        "pd.DataFrame(results).to_csv(out, index=False)\n",
        "print(f\"✅ NEOWISE done: {len(results)} rows → {out}\")\n"
      ],
      "metadata": {
        "id": "ssObBcr3QRHK"
      },
      "id": "ssObBcr3QRHK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Parallel AllWISE IRSA → inputs/wise_photometry.csv\n",
        "import os\n",
        "import pandas as pd\n",
        "from astropy.coordinates import SkyCoord\n",
        "import astropy.units as u\n",
        "from astroquery.irsa import Irsa\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "os.makedirs(\"inputs\", exist_ok=True)\n",
        "out = \"inputs/wise_photometry.csv\"\n",
        "if os.path.exists(out):\n",
        "    os.remove(out)\n",
        "\n",
        "pl = pd.read_csv(\"inputs/confirmed_planets.csv\")\n",
        "\n",
        "def query_wise(row):\n",
        "    coord = SkyCoord(ra=row[\"ra\"]*u.deg, dec=row[\"dec\"]*u.deg, frame=\"icrs\")\n",
        "    try:\n",
        "        tbl = Irsa.query_region(\n",
        "            coord,\n",
        "            catalog=\"allwise_p3as_psd\",\n",
        "            radius=5*u.arcsec,\n",
        "            columns=[\"w1mpro\",\"w2mpro\"]\n",
        "        )\n",
        "        if len(tbl):\n",
        "            w1, w2 = float(tbl[0][\"w1mpro\"]), float(tbl[0][\"w2mpro\"])\n",
        "            return {\n",
        "                \"pl_name\": row[\"pl_name\"],\n",
        "                \"w1mpro\": w1,\n",
        "                \"w2mpro\": w2,\n",
        "                \"ir_flag\": bool((w2 - w1) > 0.5)\n",
        "            }\n",
        "    except:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "results = []\n",
        "with ThreadPoolExecutor(max_workers=16) as exe:\n",
        "    futures = {exe.submit(query_wise, r): r for _, r in pl.iterrows()}\n",
        "    for f in as_completed(futures):\n",
        "        r = f.result()\n",
        "        if r:\n",
        "            results.append(r)\n",
        "\n",
        "pd.DataFrame(results).to_csv(out, index=False)\n",
        "print(f\"✅ AllWISE complete: {len(results)} rows → {out}\")\n"
      ],
      "metadata": {
        "id": "ahjWGmSPSMT6"
      },
      "id": "ahjWGmSPSMT6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Threaded Pan‑STARRS & SDSS → inputs/panstarrs.csv & inputs/sdss.csv\n",
        "import os\n",
        "import pandas as pd\n",
        "from astroquery.vizier import Vizier\n",
        "from astropy.coordinates import SkyCoord\n",
        "import astropy.units as u\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "os.makedirs(\"inputs\", exist_ok=True)\n",
        "pl = pd.read_csv(\"inputs/confirmed_planets.csv\")\n",
        "Vizier.ROW_LIMIT = 1\n",
        "\n",
        "def query_ps_sd(row):\n",
        "    coord = SkyCoord(ra=row[\"ra\"]*u.deg, dec=row[\"dec\"]*u.deg, frame=\"icrs\")\n",
        "    out = {\"pl_name\": row[\"pl_name\"], \"ps_flag\": False, \"sdss_flag\": False}\n",
        "    try:\n",
        "        ps = Vizier(catalog=\"II/349/ps1\", columns=[\"gmag\"]) \\\n",
        "             .query_region(coord, radius=5*u.arcsec)\n",
        "        if ps and len(ps[0]) > 0:\n",
        "            out[\"ps_flag\"] = True\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        sd = Vizier(catalog=\"V/147\", columns=[\"u\"]) \\\n",
        "             .query_region(coord, radius=5*u.arcsec)\n",
        "        if sd and len(sd[0]) > 0:\n",
        "            out[\"sdss_flag\"] = True\n",
        "    except:\n",
        "        pass\n",
        "    return out\n",
        "\n",
        "results = []\n",
        "with ThreadPoolExecutor(max_workers=16) as exe:\n",
        "    futures = {exe.submit(query_ps_sd, r): r for _, r in pl.iterrows()}\n",
        "    for f in as_completed(futures):\n",
        "        results.append(f.result())\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df[df.ps_flag].to_csv(\"inputs/panstarrs.csv\", index=False)\n",
        "df[df.sdss_flag].to_csv(\"inputs/sdss.csv\",     index=False)\n",
        "print(f\"✅ PS1 hits: {df.ps_flag.sum()}, SDSS hits: {df.sdss_flag.sum()}\")\n"
      ],
      "metadata": {
        "id": "Fi7LtM9RQUuy"
      },
      "id": "Fi7LtM9RQUuy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Robust Download & Filter of Breakthrough Listen hits (with DNS‐error fallback)\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# ensure inputs/ exists\n",
        "os.makedirs(\"inputs\", exist_ok=True)\n",
        "\n",
        "# URLs & paths\n",
        "url      = \"https://public.breakthroughlisten.org/dataset/hits.csv\"\n",
        "in_path  = \"inputs/bl_hits.csv\"\n",
        "out_path = \"inputs/bl_hits_filtered.csv\"\n",
        "\n",
        "# 1) Download (with retry) or skip on failure\n",
        "if not os.path.exists(in_path):\n",
        "    print(f\"➡️ Downloading BL hits from {url}\")\n",
        "    session = requests.Session()\n",
        "    session.mount(\"https://\", requests.adapters.HTTPAdapter(max_retries=3))\n",
        "    try:\n",
        "        with session.get(url, stream=True, timeout=30) as r:\n",
        "            r.raise_for_status()\n",
        "            with open(in_path, \"wb\") as f:\n",
        "                for chunk in r.iter_content(chunk_size=10_000_000):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "        print(f\"✅ Download complete: {in_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  Could not download BL hits (DNS or network error): {e}\")\n",
        "        # create a stub so downstream reads an empty file\n",
        "        pd.DataFrame(columns=[\"frequency_mhz\"]).to_csv(in_path, index=False)\n",
        "        print(f\"ℹ️  Created empty stub: {in_path}\")\n",
        "else:\n",
        "    print(f\"ℹ️ Found existing BL file, skipping download: {in_path}\")\n",
        "\n",
        "# 2) Filter into the 1419–1421 MHz window, or create empty if stubbed\n",
        "print(f\"➡️ Filtering hits between 1419–1421 MHz → {out_path}\")\n",
        "if os.path.exists(out_path):\n",
        "    os.remove(out_path)\n",
        "\n",
        "try:\n",
        "    for chunk in pd.read_csv(in_path, usecols=[\"frequency_mhz\"], chunksize=500_000):\n",
        "        mask = chunk[\"frequency_mhz\"].between(1419, 1421)\n",
        "        if mask.any():\n",
        "            chunk.loc[mask].to_csv(\n",
        "                out_path,\n",
        "                mode=\"a\",\n",
        "                index=False,\n",
        "                header=not os.path.exists(out_path)\n",
        "            )\n",
        "        print(f\"   Processed {len(chunk):,} rows, kept {mask.sum():,}\")\n",
        "    print(f\"✅ BL filtering complete: {out_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️  BL filtering skipped (no valid input): {e}\")\n",
        "    pd.DataFrame(columns=[\"frequency_mhz\"]).to_csv(out_path, index=False)\n",
        "    print(f\"ℹ️  Created empty stub: {out_path}\")\n"
      ],
      "metadata": {
        "id": "RxT6zLjCQWr8"
      },
      "id": "RxT6zLjCQWr8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Threaded Gaia RUWE → inputs/gaia_flags.csv\n",
        "import pandas as pd\n",
        "from astroquery.gaia import Gaia\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "pl = pd.read_csv(\"inputs/confirmed_planets.csv\")\n",
        "\n",
        "def query_gaia(row):\n",
        "    ra, dec, name = row[\"ra\"], row[\"dec\"], row[\"pl_name\"]\n",
        "    q = f\"\"\"\n",
        "      SELECT ruwe\n",
        "      FROM gaiaedr3.gaia_source\n",
        "      WHERE CONTAINS(\n",
        "        POINT('ICRS',ra,dec),\n",
        "        CIRCLE('ICRS',{ra},{dec},0.00027778)\n",
        "      )=1\n",
        "      LIMIT 1\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = Gaia.launch_job(q).get_results().to_pandas()\n",
        "        ru = df[\"ruwe\"].iloc[0] if not df.empty else None\n",
        "        return {\"pl_name\":name, \"gaia_flag\": bool(ru and ru>1.4)}\n",
        "    except:\n",
        "        return {\"pl_name\":name, \"gaia_flag\": False}\n",
        "\n",
        "results = []\n",
        "with ThreadPoolExecutor(max_workers=16) as exe:\n",
        "    futures = {exe.submit(query_gaia, r): r for _, r in pl.iterrows()}\n",
        "    for f in as_completed(futures):\n",
        "        results.append(f.result())\n",
        "\n",
        "pd.DataFrame(results).to_csv(\"inputs/gaia_flags.csv\", index=False)\n",
        "print(f\"✅ Gaia RUWE flags: {sum(r['gaia_flag'] for r in results)} true\")\n"
      ],
      "metadata": {
        "id": "_pIV0DabQYc-"
      },
      "id": "_pIV0DabQYc-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Retrieve one HARPS Phase‑3 dataset and flag spikes in any CSV spectra\n",
        "from astroquery.eso import Eso\n",
        "import glob, os, pandas as pd\n",
        "\n",
        "# 1) Query the ESO Phase‑3 archive for HARPS\n",
        "eso = Eso()\n",
        "tbl = eso.query_surveys(surveys=\"HARPS\")  # list Phase‑3 products for HARPS :contentReference[oaicite:0]{index=0}\n",
        "\n",
        "# ensure output folder exists\n",
        "os.makedirs(\"inputs/harps\", exist_ok=True)\n",
        "\n",
        "# 2) If nothing found, write empty stub\n",
        "if tbl is None or len(tbl) == 0:\n",
        "    print(\"ℹ️  No HARPS Phase‑3 products found; creating empty spec_flags.csv\")\n",
        "    pd.DataFrame(columns=[\"file\",\"spike_flag\",\"spike_count\"])\\\n",
        "      .to_csv(\"inputs/spec_flags.csv\", index=False)\n",
        "\n",
        "else:\n",
        "    # 3) Retrieve the first product ID\n",
        "    first_id = tbl[\"Product ID\"][0]\n",
        "    files = eso.retrieve_data(first_id, destination=\"inputs/harps\", unzip=True)  # download files :contentReference[oaicite:1]{index=1}\n",
        "\n",
        "    # 4) Loop through any CSVs in inputs/harps/, flag spikes\n",
        "    flags = []\n",
        "    for fpath in glob.glob(\"inputs/harps/*.csv\"):\n",
        "        df = pd.read_csv(fpath)\n",
        "        spike_count = (df[\"flux\"] > 5 * df[\"flux\"].median()).sum()\n",
        "        flags.append({\n",
        "            \"file\":        os.path.basename(fpath),\n",
        "            \"spike_flag\":  bool(spike_count > 0),\n",
        "            \"spike_count\": int(spike_count)\n",
        "        })\n",
        "\n",
        "    # 5) If no CSVs produced, stub it; else save real flags\n",
        "    if not flags:\n",
        "        print(\"⚠️  Retrieved HARPS files but found no CSV spectra; writing empty spec_flags.csv\")\n",
        "        pd.DataFrame(columns=[\"file\",\"spike_flag\",\"spike_count\"])\\\n",
        "          .to_csv(\"inputs/spec_flags.csv\", index=False)\n",
        "    else:\n",
        "        pd.DataFrame(flags)\\\n",
        "          .to_csv(\"inputs/spec_flags.csv\", index=False)\n",
        "        print(f\"✅ HARPS flags for {len(flags)} spectra\")\n"
      ],
      "metadata": {
        "id": "VJvbPJMwQeKH"
      },
      "id": "VJvbPJMwQeKH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}