{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: ensure inputs/ hierarchy exists\n",
        "import os\n",
        "os.makedirs(\"inputs/tess\",  exist_ok=True)\n",
        "os.makedirs(\"inputs/harps\", exist_ok=True)\n",
        "print(\"✅ inputs/ folder ready\")\n"
      ],
      "metadata": {
        "id": "NLeE4hmM8Av3",
        "outputId": "fe0f2130-a014-4f24-b899-dfaad7457d9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "NLeE4hmM8Av3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'technosignature-pipeline-v2'...\n",
            "remote: Enumerating objects: 81, done.\u001b[K\n",
            "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 81 (delta 28), reused 23 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (81/81), 28.50 KiB | 2.04 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = (\n",
        "    \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync\"\n",
        "    \"?query=select+pl_name,ra,dec,tic_id+from+pscomppars&format=csv\"\n",
        ")\n",
        "pl = pd.read_csv(url)\n",
        "pl.to_csv(\"inputs/confirmed_planets.csv\", index=False)\n",
        "print(f\"✅ Confirmed planets: {len(pl)} rows\")\n"
      ],
      "metadata": {
        "id": "KDq5iigJQKK1"
      },
      "id": "KDq5iigJQKK1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Parallel AllWISE IRSA → inputs/wise_photometry.csv\n",
        "import os, pandas as pd\n",
        "from astropy.coordinates import SkyCoord\n",
        "import astropy.units as u\n",
        "from astroquery.irsa import Irsa\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# Prepare output\n",
        "os.makedirs(\"inputs\", exist_ok=True)\n",
        "out = \"inputs/wise_photometry.csv\"\n",
        "if os.path.exists(out): os.remove(out)\n",
        "\n",
        "# Load targets\n",
        "pl = pd.read_csv(\"inputs/confirmed_planets.csv\")\n",
        "\n",
        "def query_wise(row):\n",
        "    coord = SkyCoord(ra=row[\"ra\"]*u.deg, dec=row[\"dec\"]*u.deg, frame=\"icrs\")\n",
        "    try:\n",
        "        tbl = Irsa.query_region(\n",
        "            coord,\n",
        "            catalog=\"allwise_p3as_psd\",\n",
        "            radius=5*u.arcsec,\n",
        "            columns=[\"ra\",\"dec\",\"w1mpro\",\"w2mpro\"]\n",
        "        )\n",
        "        if len(tbl) > 0:\n",
        "            hit = tbl[0]\n",
        "            w1, w2 = hit[\"w1mpro\"], hit[\"w2mpro\"]\n",
        "            return {\n",
        "                \"pl_name\": row[\"pl_name\"],\n",
        "                \"ra\": float(hit[\"ra\"]),\n",
        "                \"dec\": float(hit[\"dec\"]),\n",
        "                \"w1mpro\": float(w1),\n",
        "                \"w2mpro\": float(w2),\n",
        "                \"ir_flag\": bool((w2 - w1) > 0.5)\n",
        "            }\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "# Run 4 parallel workers\n",
        "with Pool(4) as p:\n",
        "    results = p.map(query_wise, [r for _, r in pl.iterrows()])\n",
        "\n",
        "# Filter out Nones and save\n",
        "df_wise = pd.DataFrame([r for r in results if r])\n",
        "df_wise.to_csv(out, index=False)\n",
        "print(f\"✅ AllWISE complete: {len(df_wise)} rows written to {out}\")\n"
      ],
      "metadata": {
        "id": "ssObBcr3QRHK"
      },
      "id": "ssObBcr3QRHK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Parallel Pan‑STARRS & SDSS → inputs/panstarrs.csv & inputs/sdss.csv\n",
        "import os, pandas as pd\n",
        "from astroquery.vizier import Vizier\n",
        "from astropy.coordinates import SkyCoord\n",
        "import astropy.units as u\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# Load targets\n",
        "pl = pd.read_csv(\"inputs/confirmed_planets.csv\")\n",
        "Vizier.ROW_LIMIT = 1  # only need first match\n",
        "\n",
        "def query_ps_sd(row):\n",
        "    coord = SkyCoord(ra=row[\"ra\"]*u.deg, dec=row[\"dec\"]*u.deg, frame=\"icrs\")\n",
        "    out = {\"pl_name\": row[\"pl_name\"], \"ps_flag\": False, \"sdss_flag\": False}\n",
        "    try:\n",
        "        ps = Vizier(columns=[\"gmag\"], catalog=\"II/349/ps1\") \\\n",
        "             .query_region(coord, radius=5*u.arcsec)\n",
        "        if ps and len(ps[0]) > 0:\n",
        "            out[\"ps_flag\"] = True\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        sd = Vizier(columns=[\"u\"], catalog=\"V/147\") \\\n",
        "             .query_region(coord, radius=5*u.arcsec)\n",
        "        if sd and len(sd[0]) > 0:\n",
        "            out[\"sdss_flag\"] = True\n",
        "    except:\n",
        "        pass\n",
        "    return out\n",
        "\n",
        "with Pool(4) as p:\n",
        "    hits = p.map(query_ps_sd, [r for _, r in pl.iterrows()])\n",
        "\n",
        "df_hits = pd.DataFrame(hits)\n",
        "os.makedirs(\"inputs\", exist_ok=True)\n",
        "df_hits[df_hits.ps_flag].to_csv(\"inputs/panstarrs.csv\", index=False)\n",
        "df_hits[df_hits.sdss_flag].to_csv(\"inputs/sdss.csv\",     index=False)\n",
        "print(f\"✅ Pan-STARRS hits: {df_hits.ps_flag.sum()}, SDSS hits: {df_hits.sdss_flag.sum()}\")\n"
      ],
      "metadata": {
        "id": "ahjWGmSPSMT6"
      },
      "id": "ahjWGmSPSMT6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Download & filter Breakthrough Listen hits CSV robustly\n",
        "import os\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Ensure inputs folder exists\n",
        "os.makedirs(\"inputs\", exist_ok=True)\n",
        "\n",
        "# 2) Download the full hits CSV if we don't already have it\n",
        "url = \"https://public.breakthroughlisten.org/dataset/hits.csv\"\n",
        "in_path  = \"inputs/bl_hits.csv\"\n",
        "out_path = \"inputs/bl_filtered.csv\"\n",
        "\n",
        "if not os.path.exists(in_path):\n",
        "    try:\n",
        "        print(\"➡️  Downloading BL hits via wget…\")\n",
        "        subprocess.run(\n",
        "            [\"wget\", \"-O\", in_path, url],\n",
        "            check=True\n",
        "        )\n",
        "    except Exception as e_wget:\n",
        "        print(f\"wget failed: {e_wget}\\n➡️  Trying curl…\")\n",
        "        try:\n",
        "            subprocess.run(\n",
        "                [\"curl\", \"-L\", \"-o\", in_path, url],\n",
        "                check=True\n",
        "            )\n",
        "        except Exception as e_curl:\n",
        "            raise RuntimeError(f\"Both wget and curl failed:\\n {e_wget}\\n {e_curl}\")\n",
        "\n",
        "    print(f\"✅ Download complete: {in_path}\")\n",
        "else:\n",
        "    print(f\"ℹ️  Already have {in_path}, skipping download\")\n",
        "\n",
        "# 3) Stream‑filter to 1 419–1 421 MHz\n",
        "if os.path.exists(out_path):\n",
        "    os.remove(out_path)\n",
        "\n",
        "print(\"➡️  Filtering BL hits for 1419–1421 MHz…\")\n",
        "for chunk in pd.read_csv(in_path, chunksize=1_000_000):\n",
        "    mask = chunk[\"frequency_mhz\"].between(1419, 1421)\n",
        "    if mask.any():\n",
        "        chunk.loc[mask].to_csv(\n",
        "            out_path,\n",
        "            mode=\"a\",\n",
        "            index=False,\n",
        "            header=not os.path.exists(out_path)\n",
        "        )\n",
        "    print(f\"   Processed {len(chunk):,} rows, kept {mask.sum():,}\")\n",
        "\n",
        "print(f\"✅ BL filtering complete: {out_path}\")\n"
      ],
      "metadata": {
        "id": "Fi7LtM9RQUuy"
      },
      "id": "Fi7LtM9RQUuy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Parallel Gaia RUWE → inputs/gaia_flags.csv\n",
        "import pandas as pd\n",
        "from astroquery.gaia import Gaia\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# Load targets\n",
        "pl = pd.read_csv(\"inputs/confirmed_planets.csv\")\n",
        "\n",
        "def query_gaia(row):\n",
        "    ra, dec, name = row[\"ra\"], row[\"dec\"], row[\"pl_name\"]\n",
        "    q = f\"\"\"\n",
        "        SELECT ruwe\n",
        "        FROM gaiaedr3.gaia_source\n",
        "        WHERE CONTAINS(\n",
        "          POINT('ICRS',ra,dec),\n",
        "          CIRCLE('ICRS',{ra},{dec},0.00027778)\n",
        "        )=1\n",
        "        LIMIT 1\n",
        "    \"\"\"\n",
        "    try:\n",
        "        res = Gaia.launch_job(q).get_results().to_pandas()\n",
        "        ruwe = res[\"ruwe\"].iloc[0] if not res.empty else None\n",
        "        return {\"pl_name\": name, \"gaia_flag\": bool(ruwe and ruwe > 1.4)}\n",
        "    except:\n",
        "        return {\"pl_name\": name, \"gaia_flag\": False}\n",
        "\n",
        "with Pool(4) as p:\n",
        "    flags = p.map(query_gaia, [r for _, r in pl.iterrows()])\n",
        "\n",
        "df_gaia = pd.DataFrame(flags)\n",
        "df_gaia.to_csv(\"inputs/gaia_flags.csv\", index=False)\n",
        "print(f\"✅ Gaia RUWE flags complete: {df_gaia.gaia_flag.sum()} true\")\n"
      ],
      "metadata": {
        "id": "RxT6zLjCQWr8"
      },
      "id": "RxT6zLjCQWr8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from astroquery.gaia import Gaia\n",
        "import pandas as pd\n",
        "\n",
        "pl = pd.read_csv(\"inputs/confirmed_planets.csv\")\n",
        "gaia_flags = []\n",
        "\n",
        "for _, row in pl.iterrows():\n",
        "    ra,dec = row[\"ra\"], row[\"dec\"]\n",
        "    q = f\"\"\"\n",
        "        SELECT ruwe\n",
        "        FROM gaiaedr3.gaia_source\n",
        "        WHERE CONTAINS(\n",
        "          POINT('ICRS',ra,dec),\n",
        "          CIRCLE('ICRS',{ra},{dec},0.00027778)\n",
        "        )=1 LIMIT 1\n",
        "    \"\"\"\n",
        "    try:\n",
        "        res = Gaia.launch_job(q).get_results().to_pandas()\n",
        "        ruwe = res[\"ruwe\"].iloc[0] if not res.empty else None\n",
        "        gaia_flags.append({\n",
        "            \"pl_name\":  row[\"pl_name\"],\n",
        "            \"gaia_flag\": bool(ruwe and ruwe>1.4),\n",
        "            \"ruwe\":      float(ruwe) if ruwe else None\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Gaia failed for {row['pl_name']}: {e}\")\n",
        "\n",
        "pd.DataFrame(gaia_flags).to_csv(\"inputs/gaia_flags.csv\", index=False)\n",
        "print(f\"✅ Gaia flags for {len(gaia_flags)} planets\")\n"
      ],
      "metadata": {
        "id": "_pIV0DabQYc-"
      },
      "id": "_pIV0DabQYc-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from astroquery.eso import Eso\n",
        "import pandas as pd, glob, os\n",
        "\n",
        "# 1) Retrieve one HARPS dataset (example)\n",
        "eso = Eso()\n",
        "# eso.login()  # if you have ESO credentials\n",
        "prod = eso.query_program(\"HARPS\")[0]\n",
        "eso.retrieve_data(prod, output_dir=\"inputs/harps/\")\n",
        "\n",
        "# 2) Stream all CSVs, flag spikes\n",
        "flags = []\n",
        "for f in glob.glob(\"inputs/harps/*.csv\"):\n",
        "    df = pd.read_csv(f)\n",
        "    spike_count = (df[\"flux\"] > 5*df[\"flux\"].median()).sum()\n",
        "    flags.append({\"file\": os.path.basename(f),\n",
        "                  \"spike_flag\": bool(spike_count>0),\n",
        "                  \"spike_count\": int(spike_count)})\n",
        "pd.DataFrame(flags).to_csv(\"inputs/spec_flags.csv\", index=False)\n",
        "print(f\"✅ HARPS flags for {len(flags)} files\")\n"
      ],
      "metadata": {
        "id": "VJvbPJMwQeKH"
      },
      "id": "VJvbPJMwQeKH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}